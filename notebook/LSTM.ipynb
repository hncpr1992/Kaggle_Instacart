{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load package...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define parameters...\n",
      "Load data...\n",
      "Train and test split...\n",
      "Train and val split...\n",
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# This model structure is based on \n",
    "# Next Basket Recommendation with Neural Networks \n",
    "# http://ceur-ws.org/Vol-1441/recsys2015_poster15.pdf\n",
    "\n",
    "######################################\n",
    "# import packages\n",
    "######################################\n",
    "print(\"Load package...\")\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, merge, LSTM\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "sys.path.append(\"../code\")\n",
    "from F_score import f_score\n",
    "\n",
    "\n",
    "######################################\n",
    "# Denfine parameters and constants\n",
    "######################################\n",
    "# define constants and parameters\n",
    "print(\"Define parameters...\")\n",
    "INPUT_DIR = '../input/'\n",
    "TRANSFORM_DIR = \"../transform/\"\n",
    "OUTOUT_DIR = \"../output/\"\n",
    "PRIOR_DATA_FILE = TRANSFORM_DIR + 'orders_prior_merge.csv'\n",
    "TRAIN_DATA_FILE = TRANSFORM_DIR + 'orders_train_merge.csv'\n",
    "TEST_DATA_FILE = TRANSFORM_DIR + 'orders_test.csv'\n",
    "\n",
    "VALIDATION_SPLIT_RATIO = 0.1\n",
    "\n",
    "num_dense = np.random.randint(100, 120)\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "STAMP = 'nn_%d_%.2f'%(num_dense, rate_drop_dense)\n",
    "\n",
    "\n",
    "######################################\n",
    "# load data\n",
    "######################################\n",
    "print(\"Load data...\")\n",
    "orders_train_merge = pd.read_csv(TRAIN_DATA_FILE)\n",
    "orders_prior_merge = pd.read_csv(PRIOR_DATA_FILE)\n",
    "orders_test = pd.read_csv(TEST_DATA_FILE)\n",
    "products = pd.read_csv(INPUT_DIR+\"products.csv\")\n",
    "\n",
    "# Calculate constants from data\n",
    "MAX_ITEM = max(max(orders_train_merge[\"product_id\"].apply(lambda x:len(ast.literal_eval(x)))), \n",
    "               max(orders_prior_merge[\"product_id\"].apply(lambda x:len(ast.literal_eval(x)))))\n",
    "PRE_BASKET = min(min(orders_train_merge.order_number), min(orders_test.order_number))-1\n",
    "\n",
    "\n",
    "######################################\n",
    "# Train and test split\n",
    "######################################\n",
    "print(\"Train and test split...\")\n",
    "# get useful columns\n",
    "prior = orders_prior_merge[[\"order_id\",\"user_id\",\"product_id\"]]\n",
    "train = orders_train_merge[[\"order_id\",\"user_id\",\"product_id\"]]\n",
    "test = orders_test[[\"order_id\",\"user_id\"]]\n",
    "\n",
    "# create user_id dict to accomodate users in train and test\n",
    "train_usr_dict = dict.fromkeys(train.user_id)\n",
    "test_usr_dict = dict.fromkeys(test.user_id)\n",
    "\n",
    "# split train and test prior data\n",
    "prior_train = prior.ix[[x in train_usr_dict for x in list(prior.user_id)],:]\n",
    "prior_test = prior.ix[[x in test_usr_dict for x in list(prior.user_id)],:]\n",
    "\n",
    "\n",
    "######################################\n",
    "# Create train and val set based on train above\n",
    "######################################\n",
    "print(\"Train and val split...\")\n",
    "# Train Validation split\n",
    "def split_train_val(data, val_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    val_set_size = int(len(data)*val_ratio)\n",
    "    val_indices = shuffled_indices[:val_set_size]\n",
    "    train_indices = shuffled_indices[val_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[val_indices]\n",
    "\n",
    "np.random.seed(1992)\n",
    "train_train, train_val = split_train_val(train, VALIDATION_SPLIT_RATIO)\n",
    "train_train = train_train.sort_index()\n",
    "train_val = train_val.sort_index()\n",
    "\n",
    "# create user_id dict for train_train and train_val\n",
    "train_train_usr_dict = dict.fromkeys(train_train.user_id)\n",
    "train_val_usr_dict = dict.fromkeys(train_val.user_id)\n",
    "\n",
    "# split train_train and train_val prior data\n",
    "prior_train_train = prior_train.ix[[x in train_train_usr_dict for x in list(prior_train.user_id)],:]\n",
    "prior_train_val = prior_train.ix[[x in train_val_usr_dict for x in list(prior_train.user_id)],:]\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# Preprocessing data for modeling\n",
    "######################################\n",
    "print(\"Preprocessing data...\")\n",
    "# prior\n",
    "def get_pre_baskets(prior):\n",
    "    return prior.groupby(\"user_id\").tail(3)\n",
    "\n",
    "train_train_baskets = get_pre_baskets(prior_train_train)\n",
    "train_val_baskets = get_pre_baskets(prior_train_val)\n",
    "test_baskets = get_pre_baskets(prior_test)\n",
    "\n",
    "assert min(train_val_baskets.groupby(\"user_id\").apply(len)) == PRE_BASKET\n",
    "assert min(train_train_baskets.groupby(\"user_id\").apply(len)) == PRE_BASKET\n",
    "\n",
    "# extract sequence of items from pandas frame\n",
    "sequence_train_train = list(map(ast.literal_eval,train_train_baskets[\"product_id\"].tolist()))\n",
    "sequence_train_val = list(map(ast.literal_eval,train_val_baskets[\"product_id\"].tolist()))\n",
    "sequence_test = list(map(ast.literal_eval,test_baskets[\"product_id\"].tolist()))\n",
    "\n",
    "# pad all train and val with max items\n",
    "data_train_train = pad_sequences(sequence_train_train, maxlen=MAX_ITEM, padding='post', truncating='post')\n",
    "data_train_val = pad_sequences(sequence_train_val, maxlen=MAX_ITEM, padding='post', truncating='post')\n",
    "data_test = pad_sequences(sequence_test, maxlen=MAX_ITEM, padding='post', truncating='post')\n",
    "\n",
    "# extract data for three previous purchase of each user for train, val and test\n",
    "data_train_train_bs1 = data_train_train[0::3]\n",
    "data_train_train_bs2 = data_train_train[1::3]\n",
    "data_train_train_bs3 = data_train_train[2::3]\n",
    "\n",
    "data_train_val_bs1 = data_train_val[0::3]\n",
    "data_train_val_bs2 = data_train_val[1::3]\n",
    "data_train_val_bs3 = data_train_val[2::3]\n",
    "\n",
    "data_test_bs1 = data_test[0::3]\n",
    "data_test_bs2 = data_test[1::3]\n",
    "data_test_bs3 = data_test[2::3]\n",
    "\n",
    "# extract label sequence\n",
    "label_sequence_train_train = list(map(ast.literal_eval,train_train[\"product_id\"].tolist()))\n",
    "label_sequence_train_val = list(map(ast.literal_eval,train_val[\"product_id\"].tolist()))\n",
    "\n",
    "# concatenate the three buskets to one purchase sequence for each user\n",
    "data_train_train_one_user = np.concatenate([data_train_train_bs1,data_train_train_bs2,data_train_train_bs3], axis = 1)\n",
    "mlb = MultiLabelBinarizer(classes=products[\"product_id\"].tolist(), sparse_output=True)\n",
    "\n",
    "# encode the label sequence\n",
    "train_train_label = mlb.fit_transform(label_sequence_train_train)\n",
    "train_val_label = mlb.fit_transform(label_sequence_train_val)\n",
    "\n",
    "weight_val = np.ones(MAX_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mask for cross entropy...\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# Create mask for cross entropy \n",
    "######################################\n",
    "print(\"Creating mask for cross entropy...\")\n",
    "# create user buying history\n",
    "prior_sequence = list(map(ast.literal_eval,prior[\"product_id\"].tolist()))\n",
    "prior_usr = prior[\"user_id\"]\n",
    "usr_history = {k: [] for k in set(prior_usr)}  \n",
    "\n",
    "for i in range(len(prior_usr)):\n",
    "    usr_history[prior_usr[i]].extend(prior_sequence[i])\n",
    "usr_history_encode = [usr_history[x] for x in range(1, len(usr_history)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create user list for train, val and test\n",
    "mlb = MultiLabelBinarizer(classes=range(1,49689), sparse_output=True)\n",
    "usr_history = mlb.fit_transform(usr_history_encode)\n",
    "\n",
    "# create user purchase history and separate for train, val and test\n",
    "train_train_usr = train_train_baskets.ix[0::3, \"user_id\"].values\n",
    "train_val_usr = train_val_baskets.ix[0::3, \"user_id\"].values\n",
    "test_usr = test_baskets.ix[0::3, \"user_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del prior\n",
    "del train\n",
    "del test\n",
    "del train_usr_dict\n",
    "del test_usr_dict\n",
    "del prior_train\n",
    "del prior_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Define the model graph\n",
    "######################################\n",
    "print(\"Define graph...\")\n",
    "# parameters\n",
    "EMBEDDING_DIM = 100\n",
    "num_products = len(products)+1\n",
    "\n",
    "# graph\n",
    "product_embedding_layer = Embedding(input_dim=num_products,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        embeddings_initializer='normal',\n",
    "        mask_zero=True,\n",
    "        input_length=3*MAX_ITEM,\n",
    "        trainable=True)\n",
    "\n",
    "lstm_layer = LSTM(100)\n",
    "\n",
    "bs_input = Input(shape=(3*MAX_ITEM,), dtype='int32')\n",
    "b_hist = Input(shape=(49688,), dtype='float32')\n",
    "\n",
    "embedded_bs_input = product_embedding_layer(bs_input)\n",
    "lstm_out = lstm_layer(embedded_bs_input)\n",
    "\n",
    "merged = BatchNormalization()(lstm_out)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = Dense(num_dense, kernel_initializer='normal', activation=\"relu\")(merged)\n",
    "\n",
    "merged = BatchNormalization()(lstm_out)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "preds = Dense(num_products-1, kernel_initializer='normal', activation='sigmoid')(merged)\n",
    "mask_preds = merge([preds, b_hist], mode='mul')\n",
    "\n",
    "######################################\n",
    "# Compile model \n",
    "######################################\n",
    "model = Model(inputs=[bs_input,b_hist], outputs=mask_preds)\n",
    "model.compile(loss='binary_crossentropy', \\\n",
    "        optimizer='adam')\n",
    "print(STAMP)\n",
    "print(\"The model structure is:\")\n",
    "model.summary()\n",
    "\n",
    "######################################\n",
    "# Train model \n",
    "######################################\n",
    "print(\"Start training...\")\n",
    "# set early stopping\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)  \n",
    "bst_model_path = OUTOUT_DIR + STAMP + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Batch generator\n",
    "def batch_generator(data_train_train_one_user, user_dict, buy_history, label_mat, batch_size):\n",
    "    N = np.shape(label_mat)[0]\n",
    "    number_of_batches = N/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(N)\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    while True:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        data_input = data_train_train_one_user[index_batch, :]\n",
    "        u_id = user_dict[index_batch]\n",
    "        b_hist_input = buy_history[u_id-1,:].todense()\n",
    "        label_input = np.asarray(label_mat[index_batch].todense())\n",
    "        counter += 1\n",
    "        yield([data_input, b_hist_input],label_input)\n",
    "        if (counter < number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0\n",
    "\n",
    "# Train the model\n",
    "hist = model.fit_generator(generator=batch_generator(\n",
    "                    data_train_train_one_user[0:100], train_train_usr,\\\n",
    "                    usr_history, train_train_label[0:100], 20), \\\n",
    "                    validation_data=([data_train_train_one_user[0:100], usr_history[train_train_usr[0:100]-1,:].todense()],\n",
    "                                      train_train_label[0:100].todense()), \\\n",
    "                    nb_epoch=20, steps_per_epoch=np.shape(train_train_label[0:1000])[0]/20, \\\n",
    "                    callbacks=[early_stopping, model_checkpoint]\n",
    "                   )\n",
    "\n",
    "\n",
    "# model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "# bst_val_score = min(hist.history['val_loss'])\n",
    "# usr_history[train_train_usr[0:1000],:].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction\n",
      "100/100 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "# make the prediction\n",
    "print('Making prediction')\n",
    "preds = model.predict([data_train_train_one_user[0:100], usr_history[train_train_usr[0:100],:].todense()],\n",
    "                        batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Rearrange for evaluation\n",
    "######################################"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
