{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load package...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define parameters...\n",
      "Load data...\n",
      "Train and test split...\n",
      "Train and val split...\n",
      "Preprocessing data...\n",
      "Creating mask for cross entropy...\n",
      "Define graph...\n",
      "nn_114_0.39\n",
      "The model structure is:\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 145, 100)      4968900                                      \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 300)           1200                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 114)           34314                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 114)           456                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 114)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 49688)         5714120                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 49688)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 49688)         0                                            \n",
      "====================================================================================================\n",
      "Total params: 10,718,990.0\n",
      "Trainable params: 10,718,162.0\n",
      "Non-trainable params: 828.0\n",
      "____________________________________________________________________________________________________\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peiran/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:246: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/peiran/anaconda3/lib/python3.5/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/peiran/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:296: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(steps_per_epoch=50.0, generator=<generator..., epochs=20, validation_data=([array([[..., callbacks=[<keras.ca...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 11s - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 12s - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 10s - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 10s - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 10s - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 12s - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 12s - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 11s - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 11s - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 10s - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 11s - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 11s - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 10s - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 10s - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 10s - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 12s - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 12s - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 12s - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 13s - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 13s - loss: 0.0032 - val_loss: 0.0034\n"
     ]
    }
   ],
   "source": [
    "# This model structure is based on \n",
    "# Next Basket Recommendation with Neural Networks \n",
    "# http://ceur-ws.org/Vol-1441/recsys2015_poster15.pdf\n",
    "\n",
    "######################################\n",
    "# import packages\n",
    "######################################\n",
    "print(\"Load package...\")\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, merge, LSTM\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "######################################\n",
    "# Denfine parameters and constants\n",
    "######################################\n",
    "# define constants and parameters\n",
    "print(\"Define parameters...\")\n",
    "INPUT_DIR = '../input/'\n",
    "TRANSFORM_DIR = \"../transform/\"\n",
    "OUTOUT_DIR = \"../output/\"\n",
    "PRIOR_DATA_FILE = TRANSFORM_DIR + 'orders_prior_merge.csv'\n",
    "TRAIN_DATA_FILE = TRANSFORM_DIR + 'orders_train_merge.csv'\n",
    "TEST_DATA_FILE = TRANSFORM_DIR + 'orders_test.csv'\n",
    "\n",
    "VALIDATION_SPLIT_RATIO = 0.1\n",
    "\n",
    "num_dense = np.random.randint(100, 120)\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "STAMP = 'nn_%d_%.2f'%(num_dense, rate_drop_dense)\n",
    "\n",
    "\n",
    "######################################\n",
    "# load data\n",
    "######################################\n",
    "print(\"Load data...\")\n",
    "orders_train_merge = pd.read_csv(TRAIN_DATA_FILE)\n",
    "orders_prior_merge = pd.read_csv(PRIOR_DATA_FILE)\n",
    "orders_test = pd.read_csv(TEST_DATA_FILE)\n",
    "products = pd.read_csv(INPUT_DIR+\"products.csv\")\n",
    "\n",
    "# Calculate constants from data\n",
    "MAX_ITEM = max(max(orders_train_merge[\"product_id\"].apply(lambda x:len(ast.literal_eval(x)))), \n",
    "               max(orders_prior_merge[\"product_id\"].apply(lambda x:len(ast.literal_eval(x)))))\n",
    "PRE_BASKET = min(min(orders_train_merge.order_number), min(orders_test.order_number))-1\n",
    "\n",
    "\n",
    "######################################\n",
    "# Train and test split\n",
    "######################################\n",
    "print(\"Train and test split...\")\n",
    "# get useful columns\n",
    "prior = orders_prior_merge[[\"order_id\",\"user_id\",\"product_id\"]]\n",
    "train = orders_train_merge[[\"order_id\",\"user_id\",\"product_id\"]]\n",
    "test = orders_test[[\"order_id\",\"user_id\"]]\n",
    "\n",
    "# create user_id dict to accomodate users in train and test\n",
    "train_usr_dict = dict.fromkeys(train.user_id)\n",
    "test_usr_dict = dict.fromkeys(test.user_id)\n",
    "\n",
    "# split train and test prior data\n",
    "prior_train = prior.ix[[x in train_usr_dict for x in list(prior.user_id)],:]\n",
    "prior_test = prior.ix[[x in test_usr_dict for x in list(prior.user_id)],:]\n",
    "\n",
    "\n",
    "######################################\n",
    "# Create train and val set based on train above\n",
    "######################################\n",
    "print(\"Train and val split...\")\n",
    "# Train Validation split\n",
    "def split_train_val(data, val_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    val_set_size = int(len(data)*val_ratio)\n",
    "    val_indices = shuffled_indices[:val_set_size]\n",
    "    train_indices = shuffled_indices[val_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[val_indices]\n",
    "\n",
    "np.random.seed(1992)\n",
    "train_train, train_val = split_train_val(train, VALIDATION_SPLIT_RATIO)\n",
    "\n",
    "# create user_id dict for train_train and train_val\n",
    "train_train_usr_dict = dict.fromkeys(train_train.user_id)\n",
    "train_val_usr_dict = dict.fromkeys(train_val.user_id)\n",
    "\n",
    "# split train_train and train_val prior data\n",
    "prior_train_train = prior_train.ix[[x in train_train_usr_dict for x in list(prior_train.user_id)],:]\n",
    "prior_train_val = prior_train.ix[[x in train_val_usr_dict for x in list(prior_train.user_id)],:]\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# Preprocessing data for modeling\n",
    "######################################\n",
    "print(\"Preprocessing data...\")\n",
    "# prior\n",
    "def get_pre_baskets(prior):\n",
    "    return prior.groupby(\"user_id\").tail(3)\n",
    "\n",
    "train_train_baskets = get_pre_baskets(prior_train_train)\n",
    "train_val_baskets = get_pre_baskets(prior_train_val)\n",
    "test_baskets = get_pre_baskets(prior_test)\n",
    "\n",
    "assert min(train_val_baskets.groupby(\"user_id\").apply(len)) == PRE_BASKET\n",
    "assert min(train_train_baskets.groupby(\"user_id\").apply(len)) == PRE_BASKET\n",
    "\n",
    "# extract sequence of items from pandas frame\n",
    "sequence_train_train = list(map(ast.literal_eval,train_train_baskets[\"product_id\"].tolist()))\n",
    "sequence_train_val = list(map(ast.literal_eval,train_val_baskets[\"product_id\"].tolist()))\n",
    "sequence_test = list(map(ast.literal_eval,test_baskets[\"product_id\"].tolist()))\n",
    "\n",
    "# pad all train and val with max items\n",
    "data_train_train = pad_sequences(sequence_train_train, maxlen=MAX_ITEM, padding='post', truncating='post')\n",
    "data_train_val = pad_sequences(sequence_train_val, maxlen=MAX_ITEM, padding='post', truncating='post')\n",
    "data_test = pad_sequences(sequence_test, maxlen=MAX_ITEM, padding='post', truncating='post')\n",
    "\n",
    "# extract sequence of items from pandas frame\n",
    "sequence_train_train = list(map(ast.literal_eval,train_train_baskets[\"product_id\"].tolist()))\n",
    "sequence_train_val = list(map(ast.literal_eval,train_val_baskets[\"product_id\"].tolist()))\n",
    "sequence_test = list(map(ast.literal_eval,test_baskets[\"product_id\"].tolist()))\n",
    "\n",
    "# extract data for three previous purchase of each user for train, val and test\n",
    "data_train_train_bs1 = data_train_train[0::3]\n",
    "data_train_train_bs2 = data_train_train[1::3]\n",
    "data_train_train_bs3 = data_train_train[2::3]\n",
    "\n",
    "data_train_val_bs1 = data_train_val[0::3]\n",
    "data_train_val_bs2 = data_train_val[1::3]\n",
    "data_train_val_bs3 = data_train_val[2::3]\n",
    "\n",
    "data_test_bs1 = data_test[0::3]\n",
    "data_test_bs2 = data_test[1::3]\n",
    "data_test_bs3 = data_test[2::3]\n",
    "\n",
    "# extract label sequence\n",
    "label_sequence_train_train = list(map(ast.literal_eval,train_train[\"product_id\"].tolist()))\n",
    "label_sequence_train_val = list(map(ast.literal_eval,train_val[\"product_id\"].tolist()))\n",
    "\n",
    "# concatenate the three buskets to one purchase sequence for each user\n",
    "data_train_train_one_user = np.concatenate([data_train_train_bs1,data_train_train_bs2,data_train_train_bs3], axis = 1)\n",
    "mlb = MultiLabelBinarizer(classes=products[\"product_id\"].tolist(), sparse_output=True)\n",
    "\n",
    "# encode the label sequence\n",
    "train_train_label = mlb.fit_transform(label_sequence_train_train)\n",
    "train_val_label = mlb.fit_transform(label_sequence_train_val)\n",
    "\n",
    "weight_val = np.ones(MAX_ITEM)\n",
    "\n",
    "\n",
    "######################################\n",
    "# Create mask for cross entropy \n",
    "######################################\n",
    "print(\"Creating mask for cross entropy...\")\n",
    "# create user buying history\n",
    "prior_sequence = list(map(ast.literal_eval,prior[\"product_id\"].tolist()))\n",
    "prior_usr = prior[\"user_id\"]\n",
    "usr_history = {k: [] for k in set(prior_usr)}  \n",
    "\n",
    "for i in range(len(prior_usr)):\n",
    "    usr_history[prior_usr[i]].extend(prior_sequence[i])\n",
    "usr_history_encode = [usr_history[x] for x in range(1, len(usr_history)+1)]\n",
    "\n",
    "# create user list for train, val and test\n",
    "mlb = MultiLabelBinarizer(classes=range(1,49689), sparse_output=True)\n",
    "usr_history = mlb.fit_transform(usr_history_encode)\n",
    "\n",
    "# create user purchase history and separate for train, val and test\n",
    "train_train_usr = train_train_baskets.ix[0::3, \"user_id\"].values\n",
    "train_val_usr = train_val_baskets.ix[0::3, \"user_id\"].values\n",
    "test_usr = test_baskets.ix[0::3, \"user_id\"].values\n",
    "\n",
    "del prior\n",
    "del train\n",
    "del test\n",
    "del train_usr_dict\n",
    "del test_usr_dict\n",
    "del prior_train\n",
    "del prior_test\n",
    "\n",
    "######################################\n",
    "# Define the model graph\n",
    "######################################\n",
    "print(\"Define graph...\")\n",
    "# parameters\n",
    "EMBEDDING_DIM = 100\n",
    "num_products = len(products)+1\n",
    "\n",
    "# graph\n",
    "product_embedding_layer = Embedding(input_dim=num_products,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        embeddings_initializer='normal',\n",
    "        mask_zero=True,\n",
    "        input_length=MAX_ITEM,\n",
    "        trainable=True)\n",
    "\n",
    "bs_1 = Input(shape=(MAX_ITEM,), dtype='int32')\n",
    "bs_2 = Input(shape=(MAX_ITEM,), dtype='int32')\n",
    "bs_3 = Input(shape=(MAX_ITEM,), dtype='int32')\n",
    "b_hist = Input(shape=(49688,), dtype='float32')\n",
    "\n",
    "embedded_bs_1 = product_embedding_layer(bs_1)\n",
    "embedded_bs_2 = product_embedding_layer(bs_2)\n",
    "embedded_bs_3 = product_embedding_layer(bs_3)\n",
    "\n",
    "\n",
    "def embedding_mean(e):\n",
    "    return K.mean(e, axis = 1)\n",
    "\n",
    "embedded_bs_1_mean = Lambda(embedding_mean)(embedded_bs_1)\n",
    "embedded_bs_2_mean = Lambda(embedding_mean)(embedded_bs_2)\n",
    "embedded_bs_3_mean = Lambda(embedding_mean)(embedded_bs_3)\n",
    "\n",
    "merged = concatenate([embedded_bs_1_mean, embedded_bs_2_mean, embedded_bs_3_mean])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = Dense(num_dense, kernel_initializer='normal', activation=\"relu\")(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "preds = Dense(num_products-1, kernel_initializer='normal', activation='sigmoid')(merged)\n",
    "mask_preds = merge([preds, b_hist], mode='mul')\n",
    "\n",
    "######################################\n",
    "# Compile model \n",
    "######################################\n",
    "model = Model(inputs=[bs_1,bs_2,bs_3,b_hist], outputs=mask_preds)\n",
    "model.compile(loss='binary_crossentropy', \\\n",
    "        optimizer='adam')\n",
    "print(STAMP)\n",
    "print(\"The model structure is:\")\n",
    "model.summary()\n",
    "\n",
    "######################################\n",
    "# Train model \n",
    "######################################\n",
    "print(\"Start training...\")\n",
    "# set early stopping\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)  \n",
    "bst_model_path = OUTOUT_DIR + STAMP + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Batch generator\n",
    "def batch_generator(bs_1, bs_2, bs_3, user_dict, buy_history, label_mat, batch_size):\n",
    "    N = np.shape(label_mat)[0]\n",
    "    number_of_batches = N/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(N)\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    while True:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        bs_1_batch = bs_1[index_batch, :]\n",
    "        bs_2_batch = bs_2[index_batch, :]\n",
    "        bs_3_batch = bs_3[index_batch, :]\n",
    "        u_id = user_dict[index_batch]\n",
    "        b_hist_input = buy_history[u_id,:].todense()\n",
    "        label_input = np.asarray(label_mat[index_batch].todense())\n",
    "        counter += 1\n",
    "        yield([bs_1_batch, bs_2_batch, bs_3_batch, b_hist_input],label_input)\n",
    "        if (counter < number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0\n",
    "\n",
    "# Train the model\n",
    "hist = model.fit_generator(generator=batch_generator(\n",
    "                   data_train_train_bs1[0:1000], data_train_train_bs2[0:1000],\\\n",
    "                   data_train_train_bs3[0:1000], train_train_usr, usr_history, \\\n",
    "                   train_train_label[0:1000], 20), \\\n",
    "                   validation_data=([data_train_train_bs1[0:1000], data_train_train_bs2[0:1000],\\\n",
    "                   data_train_train_bs3[0:1000], usr_history[train_train_usr[0:1000],:].todense()],  \n",
    "                   train_train_label[0:1000].todense()), \\\n",
    "                   nb_epoch=20, steps_per_epoch=np.shape(train_train_label[0:1000])[0]/20, \\\n",
    "                   callbacks=[early_stopping, model_checkpoint]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction\n",
      "1000/1000 [==============================] - 1s     \n",
      "preds\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "arg_sorted_preds\n",
      "[[    0 33112 33113 ..., 11642 29203 29087]\n",
      " [    0 33093 33094 ..., 23835 10414  1322]\n",
      " [    0 33098 33099 ..., 33488 29179 30993]\n",
      " ..., \n",
      " [    0 33106 33107 ..., 24185 11884 29645]\n",
      " [    0 33119 33120 ...,  3398 22801 42499]\n",
      " [    0 33094 33095 ..., 19154 27583 19884]]\n",
      "Max prob in each row\n",
      "[  0.00000000e+00   0.00000000e+00   3.53056844e-01   0.00000000e+00\n",
      "   0.00000000e+00   9.94748622e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.18428243e-02\n",
      "   0.00000000e+00   2.85115410e-01   8.05712342e-02   0.00000000e+00\n",
      "   4.54556607e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.81768847e-01   2.57151835e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.30388749e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.94810376e-01\n",
      "   0.00000000e+00   0.00000000e+00   2.11349741e-01   7.82967865e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.24031378e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   9.12065566e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.29708962e-02   0.00000000e+00\n",
      "   0.00000000e+00   1.24016427e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.16469771e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.77168743e-01   0.00000000e+00   9.70531348e-03   4.12095897e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.57939720e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.17467359e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.68535337e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.43817964e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.12237731e-01   2.44036801e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.55518861e-01   0.00000000e+00   0.00000000e+00   2.40549855e-02\n",
      "   0.00000000e+00   1.98737942e-02   8.68097041e-03   0.00000000e+00\n",
      "   0.00000000e+00   6.90739546e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.23968610e-01   0.00000000e+00\n",
      "   3.05508868e-01   0.00000000e+00   3.21300104e-02   0.00000000e+00\n",
      "   9.10290051e-02   2.01251339e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.62877767e-02\n",
      "   9.21021402e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   6.31470159e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.64281502e-01   0.00000000e+00   0.00000000e+00   1.59315005e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.48978034e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.54764888e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.69669595e-02   0.00000000e+00   0.00000000e+00   1.39397189e-01\n",
      "   0.00000000e+00   0.00000000e+00   1.10462137e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.44954715e-02   0.00000000e+00\n",
      "   0.00000000e+00   1.13725901e+00   6.72273189e-01   0.00000000e+00\n",
      "   1.32500459e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.63006280e-03   0.00000000e+00   1.52328018e-01\n",
      "   9.39876214e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.39404437e-01   0.00000000e+00\n",
      "   1.53536394e-01   0.00000000e+00   0.00000000e+00   7.19762594e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.33798770e-01\n",
      "   0.00000000e+00   2.48288256e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.40422728e+00   0.00000000e+00   8.86948928e-02\n",
      "   0.00000000e+00   4.06161472e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.22170401e-01\n",
      "   0.00000000e+00   1.25425816e-01   8.35597292e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.45432916e-01   5.55848397e-01   0.00000000e+00\n",
      "   0.00000000e+00   9.13901348e-03   0.00000000e+00   7.19472095e-02\n",
      "   0.00000000e+00   0.00000000e+00   1.27592478e-02   0.00000000e+00\n",
      "   8.00265335e-02   0.00000000e+00   0.00000000e+00   1.83236558e-01\n",
      "   1.48162335e-01   0.00000000e+00   0.00000000e+00   1.24350619e-01\n",
      "   2.67555695e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.38996789e-02\n",
      "   2.64199693e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.06992177e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.42754177e-02   2.23355670e-03   7.71061196e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.37150478e-01   0.00000000e+00   0.00000000e+00\n",
      "   5.38448617e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.94406256e-01   2.18659267e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   3.65153328e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.72423621e-02\n",
      "   0.00000000e+00   2.35183593e-02   0.00000000e+00   0.00000000e+00\n",
      "   9.05670114e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.77460751e-02   5.05446084e-02   1.57403070e-02\n",
      "   5.14860004e-02   3.22539324e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.27628863e-01   7.07544666e-03   0.00000000e+00\n",
      "   0.00000000e+00   3.29826474e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.45523208e-02   0.00000000e+00   0.00000000e+00\n",
      "   1.18905377e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.04525171e-02   5.25902957e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.50398976e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.76775714e-02   0.00000000e+00\n",
      "   2.25671027e-02   0.00000000e+00   1.74725354e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.29688603e-02\n",
      "   0.00000000e+00   0.00000000e+00   4.13932614e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.06021678e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.57211085e-02\n",
      "   0.00000000e+00   2.52444781e-02   3.18335630e-02   0.00000000e+00\n",
      "   0.00000000e+00   2.47975308e-02   0.00000000e+00   0.00000000e+00\n",
      "   9.96913575e-03   0.00000000e+00   0.00000000e+00   7.47913308e-03\n",
      "   2.64792610e-02   0.00000000e+00   0.00000000e+00   4.26568990e-01\n",
      "   1.72428463e-01   4.12269942e-02   0.00000000e+00   1.15218610e-01\n",
      "   0.00000000e+00   1.67416926e-01   0.00000000e+00   0.00000000e+00\n",
      "   5.30960402e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.22748928e-02   2.60641019e-01\n",
      "   0.00000000e+00   0.00000000e+00   1.84019074e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.88496211e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   4.67878953e-01   0.00000000e+00   0.00000000e+00\n",
      "   9.98889748e-03   8.35327879e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.12389769e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.07262829e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   8.39870572e-02   1.00565068e-02   0.00000000e+00\n",
      "   1.67829320e-01   0.00000000e+00   0.00000000e+00   7.11237825e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.43492669e-01   2.88205771e-01   0.00000000e+00\n",
      "   0.00000000e+00   4.69668061e-02   0.00000000e+00   2.25673360e-03\n",
      "   0.00000000e+00   9.60316136e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.30978613e-01   7.89727271e-03\n",
      "   3.17518674e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.18453053e-02   0.00000000e+00   0.00000000e+00\n",
      "   1.47442119e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.98534987e-01\n",
      "   0.00000000e+00   1.87991001e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   3.85731936e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.70350499e-01\n",
      "   0.00000000e+00   1.81530811e-01   4.23770973e-02   0.00000000e+00\n",
      "   5.91150625e-03   2.82378448e-02   7.31604695e-02   1.53845865e-02\n",
      "   0.00000000e+00   0.00000000e+00   3.06075089e-01   0.00000000e+00\n",
      "   0.00000000e+00   1.61117435e-01   0.00000000e+00   0.00000000e+00\n",
      "   4.55982573e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   8.57424363e-02   0.00000000e+00   0.00000000e+00\n",
      "   2.64936537e-02   0.00000000e+00   1.12340422e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.54310982e-03   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.35082948e-01\n",
      "   0.00000000e+00   1.90657541e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   6.17802255e-02   1.27950490e-01   0.00000000e+00\n",
      "   2.65333905e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.62828493e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.45526082e-03   0.00000000e+00   0.00000000e+00   4.34610173e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   6.48541003e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.30752076e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.41363694e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.39576476e-03   0.00000000e+00   2.54391693e-02\n",
      "   0.00000000e+00   3.74391936e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.53062798e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.36421427e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.79099930e-02\n",
      "   2.37497106e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   6.16365187e-02   0.00000000e+00   0.00000000e+00\n",
      "   1.29606381e-01   0.00000000e+00   4.10819901e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.62750706e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.56323295e-02   0.00000000e+00   5.34505323e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   6.12090647e-01   0.00000000e+00\n",
      "   0.00000000e+00   2.27748193e-02   7.77157098e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.31032185e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   6.09241230e-02   0.00000000e+00\n",
      "   2.84917634e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   7.21008539e-01   2.61424016e-02   0.00000000e+00\n",
      "   3.69972270e-03   0.00000000e+00   0.00000000e+00   7.85307363e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.33454632e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.07438022e-03   2.95298249e-02   0.00000000e+00   0.00000000e+00\n",
      "   3.29788588e-02   5.82233025e-02   0.00000000e+00   0.00000000e+00\n",
      "   4.01500650e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.78136373e-01   0.00000000e+00   0.00000000e+00   8.75884965e-02\n",
      "   2.53016811e-01   0.00000000e+00   1.44533694e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.41521429e-02   0.00000000e+00\n",
      "   4.44651917e-02   5.16757891e-02   5.70023432e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.90370897e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.98882688e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.84259755e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.33493058e-02   0.00000000e+00   3.76569293e-02   0.00000000e+00\n",
      "   5.42076513e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   4.03293706e-02   0.00000000e+00   3.70928777e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.00992846e-02   0.00000000e+00   1.07883938e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.93531600e-02   0.00000000e+00   4.56341114e-02   1.44967832e-01\n",
      "   2.67419666e-01   0.00000000e+00   0.00000000e+00   2.17502047e-02\n",
      "   0.00000000e+00   0.00000000e+00   3.21404152e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   3.11127845e-02   0.00000000e+00\n",
      "   3.05029869e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.15732542e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.43500972e-02   1.38025926e-02   0.00000000e+00\n",
      "   0.00000000e+00   1.47586793e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.11843178e-02   7.53527144e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.02489987e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.22516803e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.37435630e-01\n",
      "   3.54669057e-02   0.00000000e+00   1.83135604e-01   0.00000000e+00\n",
      "   5.57453819e-02   0.00000000e+00   0.00000000e+00   4.99265417e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.11816496e-02   0.00000000e+00   3.28068323e-02   0.00000000e+00\n",
      "   6.24277536e-02   4.27695327e-02   0.00000000e+00   3.53658002e-01\n",
      "   2.49115253e-01   1.34687111e-01   0.00000000e+00   0.00000000e+00\n",
      "   4.48000804e-02   1.84024144e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.63015664e-01   0.00000000e+00\n",
      "   0.00000000e+00   8.33996460e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.37790591e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.14170164e-01   1.95468962e-02   0.00000000e+00\n",
      "   4.92270876e-01   0.00000000e+00   0.00000000e+00   1.58246383e-01\n",
      "   7.38339312e-02   0.00000000e+00   1.54917017e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.60385399e-02   0.00000000e+00\n",
      "   0.00000000e+00   8.12256932e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   9.62299697e-01\n",
      "   0.00000000e+00   5.39921343e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   7.34003615e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   4.55056261e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.36856654e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.12821171e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.26987049e-03   0.00000000e+00\n",
      "   0.00000000e+00   1.81544330e-02   0.00000000e+00   2.80578937e-02\n",
      "   5.79970106e-02   3.56695689e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.69209056e-01   2.44571622e-02   0.00000000e+00\n",
      "   6.47268295e-02   6.94542625e-01   6.09905040e-03   0.00000000e+00\n",
      "   2.72027601e-01   0.00000000e+00   2.77234316e-01   0.00000000e+00\n",
      "   0.00000000e+00   2.93505900e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.26579554e-01   0.00000000e+00\n",
      "   3.96686010e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.01404914e-02   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "# bst_val_score = min(hist.history['val_loss'])\n",
    "# usr_history[train_train_usr[0:1000],:].todense()\n",
    "\n",
    "# make the prediction\n",
    "print('Making prediction')\n",
    "preds = model.predict([data_train_train_bs1[1000:2000], data_train_train_bs2[1000:2000],\n",
    "                        data_train_train_bs3[1000:2000], usr_history[train_train_usr[1000:2000],:].todense()],\n",
    "                        batch_size=128, verbose=1)\n",
    "\n",
    "print(\"preds\")\n",
    "print(preds)\n",
    "\n",
    "print(\"arg_sorted_preds\")\n",
    "print(np.argsort(preds))\n",
    "\n",
    "print(\"Max prob in each row\")\n",
    "trueth = np.array(train_train_label[0:1000].todense())\n",
    "print((trueth * preds).sum(axis = 1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
